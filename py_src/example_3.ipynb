{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd538e03",
   "metadata": {},
   "source": [
    "# Dependence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1d9920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from pyDOE import lhs\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.tri as mtri\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abf3723",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b916cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 10000\n",
    "layers = np.array([2,32,64,32,2])\n",
    "lr = 0.001\n",
    "\n",
    "Nf = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f7812d",
   "metadata": {},
   "source": [
    "# Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e6465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(V,tris,U):\n",
    "    _,ax=plt.subplots()\n",
    "    ax.tripcolor(V[:,0],V[:,1],tris,U,cmap='viridis')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('U')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877836c",
   "metadata": {},
   "source": [
    "# Build initial physical mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa76091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rows = 20\n",
    "m_cols = 20\n",
    "\n",
    "x = torch.linspace(0, 1, m_cols + 1)\n",
    "y = torch.linspace(0, 1, m_rows + 1)\n",
    "x = x.repeat(m_rows + 1)\n",
    "y = y.repeat_interleave(m_cols + 1)\n",
    "V = torch.stack([x,y],dim=1)\n",
    "\n",
    "tris = []\n",
    "\n",
    "for i in range(m_cols):\n",
    "   for j in range(m_rows):\n",
    "       tris.append([j * (m_cols + 1) + i, j * (m_cols + 1) + i + 1, (j + 1) * (m_cols + 1) + i + 1])\n",
    "       tris.append([j * (m_cols + 1) + i, (j + 1) * (m_cols + 1) + i + 1, (j + 1) * (m_cols + 1) + i])\n",
    "        \n",
    "tris = torch.tensor(tris, dtype=torch.short)\n",
    "\n",
    "left_boundarys = V[(V[:,0]==0.0)]\n",
    "bottom_boundarys = V[(V[:,1]==0.0)]\n",
    "top_boundarys = V[(V[:,1]==1.0)]\n",
    "right_boundarys = V[(V[:,0]==1.0)]\n",
    "\n",
    "plt.triplot(x,y,tris,'k-',linewidth=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43d4a8c",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ddb788",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_V = V[(V[:,0]==0.0)]\n",
    "right_V = V[(V[:,0]==1.0)]\n",
    "bottom_V = V[(V[:,1]==0.0)]\n",
    "top_V = V[(V[:,1]==1.0)]\n",
    "\n",
    "PDE_V = V[0] + (V[-1]-V[0])*lhs(2,Nf)\n",
    "\n",
    "V = V.to(device)\n",
    "left_V = left_V.float().to(device)\n",
    "right_V = right_V.float().to(device)\n",
    "bottom_V = bottom_V.float().to(device)\n",
    "top_V = top_V.float().to(device)\n",
    "PDE_V = PDE_V.float().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9af9f",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def U(V):\n",
    "    return torch.tanh(-30*(V[:,1]-0.5-0.25*torch.sin(2*torch.pi*V[:,0])))\n",
    "\n",
    "def U_x_y(V):\n",
    "    U_x = (1-U(V)**2)*(15*torch.pi*torch.cos(2*torch.pi*V[:,0]))\n",
    "    U_y = (1-U(V)**2)*(-30)\n",
    "    return torch.stack([U_x,U_y],dim=1)\n",
    "\n",
    "def Metric(V):\n",
    "    u_x_y = U_x_y(V)\n",
    "    return torch.sqrt(0.1*torch.linalg.norm(u_x_y,dim=1)**2 + 1)\n",
    "\n",
    "# M = Metric(V)\n",
    "# Plot(V.detach().cpu(),tris.detach().cpu(),M.detach().cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1236f86",
   "metadata": {},
   "source": [
    "# Neural Network generates initial logical mesh\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "-\\nabla \\cdot (\\nabla \\xi) = 0, \\\\\n",
    "\\xi|_{\\partial \\Omega} = \\xi_b,\n",
    "\\end{cases}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698128ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN1(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self.loss_function2 = nn.L1Loss(reduction ='sum')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]) \n",
    "        self.iter = 0\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        a = x.float()\n",
    "        for i in range(len(self.layers)-2):  \n",
    "            z = self.linears[i](a)              \n",
    "            a = self.activation(z)\n",
    "        a = self.linears[-1](a)\n",
    "        return a    \n",
    "    \n",
    "    def loss_BC(self,left_V,right_V,bottom_V,top_V):\n",
    "        return self.loss_function2(self.forward(left_V),left_V) + \\\n",
    "               self.loss_function2(self.forward(right_V),right_V) + \\\n",
    "               self.loss_function2(self.forward(bottom_V),bottom_V) + \\\n",
    "               self.loss_function2(self.forward(top_V),top_V)\n",
    "              \n",
    "    def loss_PDE(self,PDE_V):\n",
    "        g = PDE_V.clone()\n",
    "        g.requires_grad = True\n",
    "        xi_eta = self.forward(g)\n",
    "        xi = xi_eta[:,0].unsqueeze(1)\n",
    "        eta = xi_eta[:,1].unsqueeze(1)\n",
    "        xi_x_y = autograd.grad(xi,g,torch.ones([g.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "        xi_xx_yy = autograd.grad(xi_x_y,g,torch.ones(g.shape).to(device),create_graph=True)[0]\n",
    "        eta_x_y = autograd.grad(eta,g,torch.ones([g.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "        eta_xx_yy = autograd.grad(eta_x_y,g,torch.ones(g.shape).to(device),create_graph=True)[0]  \n",
    "        return self.loss_function(xi_xx_yy[:,[0]]+xi_xx_yy[:,[1]],torch.zeros(g.shape[0],1).to(device)) + \\\n",
    "                self.loss_function(eta_xx_yy[:,[0]]+eta_xx_yy[:,[1]],torch.zeros(g.shape[0],1).to(device))\n",
    "\n",
    "    def loss(self,left_V,right_V,bottom_V,top_V,PDE_V):      \n",
    "        return self.loss_PDE(PDE_V) + self.loss_BC(left_V,right_V,bottom_V,top_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b2cd2c",
   "metadata": {},
   "source": [
    "# Generate initial logic mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8fa8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_0 = FCN1(layers).to(device)\n",
    "optimizer = torch.optim.Adam(xi_0.parameters(),lr=lr,amsgrad=False)\n",
    "\n",
    "for i in range(steps):\n",
    "    loss = xi_0.loss(left_V,right_V,bottom_V,top_V,PDE_V)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(i,loss.detach().cpu().numpy())\n",
    "    \n",
    "V_xi_0 = xi_0(V)\n",
    "V_xi_0_cpu = xi_0(V).detach().cpu()\n",
    "plt.triplot(V_xi_0_cpu[:,0],V_xi_0_cpu[:,1],tris.detach().cpu(),'k-',linewidth=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd20140",
   "metadata": {},
   "source": [
    "# Neural Network generate initial physical mesh by solving harmonic maps\n",
    "\n",
    "$$\n",
    "g^{ij}\\frac{\\partial}{\\partial \\xi^i}w\\frac{\\partial x^k}{\\partial \\xi^j} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "g^{ij} = \\frac{\\partial \\xi^i}{\\partial x^\\alpha}\\frac{\\partial \\xi^j}{\\partial x^\\alpha}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b344343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCN2(nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        self.loss_function2 = nn.L1Loss(reduction ='sum')\n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]) \n",
    "        self.iter = 0\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        a = x.float()\n",
    "        for i in range(len(self.layers)-2):  \n",
    "            z = self.linears[i](a)              \n",
    "            a = self.activation(z)\n",
    "        a = self.linears[-1](a)\n",
    "        return a    \n",
    "    \n",
    "    def loss_BC(self,left_V,right_V,bottom_V,top_V):\n",
    "        return self.loss_function2(self.forward(left_V),left_V) + \\\n",
    "               self.loss_function2(self.forward(right_V),right_V) + \\\n",
    "               self.loss_function2(self.forward(bottom_V),bottom_V) + \\\n",
    "               self.loss_function2(self.forward(top_V),top_V)\n",
    "    \n",
    "    def loss_PDE(self,PDE_V):\n",
    "        g = PDE_V.clone()\n",
    "        g.requires_grad = True\n",
    "        xy = self.forward(g)\n",
    "        x = xy[:,0].unsqueeze(1)\n",
    "        y = xy[:,1].unsqueeze(1)\n",
    "        M = Metric(xy).unsqueeze(1)\n",
    "        x_xi_eta = autograd.grad(x,g,torch.ones([g.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "        y_xi_eta = autograd.grad(y,g,torch.ones([g.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "        M_xi_eta = autograd.grad(M,g,torch.ones([g.shape[0],1]).to(device),create_graph=True)[0]\n",
    "        x_xi = x_xi_eta[:,[0]]\n",
    "        x_eta = x_xi_eta[:,[1]]\n",
    "        y_xi = y_xi_eta[:,[0]]\n",
    "        y_eta = y_xi_eta[:,[1]]\n",
    "        x_xixi_xieta = autograd.grad(x_xi,g,torch.ones([g.shape[0],1]).to(device),create_graph=True)[0]\n",
    "        x_etaxi_etaeta = autograd.grad(x_eta,g,torch.ones([g.shape[0],1]).to(device),create_graph=True)[0]\n",
    "        y_xixi_xieta = autograd.grad(y_xi,g,torch.ones([g.shape[0],1]).to(device),create_graph=True)[0]\n",
    "        y_etaxi_etaeta = autograd.grad(y_eta,g,torch.ones([g.shape[0],1]).to(device),create_graph=True)[0]\n",
    "        g00 = (1/x_xi_eta[:,[0]])**2+(1/y_xi_eta[:,[0]])**2\n",
    "        g01 = (1/x_xi_eta[:,[0]])*(1/x_xi_eta[:,[1]])+(1/y_xi_eta[:,[0]])*(1/y_xi_eta[:,[1]])\n",
    "        g11 = (1/x_xi_eta[:,[1]])**2+(1/y_xi_eta[:,[1]])**2\n",
    "        loss1 = g00*(M_xi_eta[:,[0]]*x_xi_eta[:,[0]]+M*x_xixi_xieta[:,[0]]) + \\\n",
    "                g01*(M_xi_eta[:,[0]]*x_xi_eta[:,[1]]+M*x_etaxi_etaeta[:,[0]]) + \\\n",
    "                g01*(M_xi_eta[:,[1]]*x_xi_eta[:,[0]]+M*x_xixi_xieta[:,[1]]) + \\\n",
    "                g11*(M_xi_eta[:,[1]]*x_xi_eta[:,[1]]+M*x_etaxi_etaeta[:,[1]])\n",
    "        loss2 = g00*(M_xi_eta[:,[0]]*y_xi_eta[:,[0]]+M*y_xixi_xieta[:,[0]]) + \\\n",
    "                g01*(M_xi_eta[:,[0]]*y_xi_eta[:,[1]]+M*y_etaxi_etaeta[:,[0]]) + \\\n",
    "                g01*(M_xi_eta[:,[1]]*y_xi_eta[:,[0]]+M*y_xixi_xieta[:,[1]]) + \\\n",
    "                g11*(M_xi_eta[:,[1]]*y_xi_eta[:,[1]]+M*y_etaxi_etaeta[:,[1]])\n",
    "        return self.loss_function(loss1,torch.zeros(g.shape[0],1).to(device)) + \\\n",
    "               self.loss_function(loss2,torch.zeros(g.shape[0],1).to(device))\n",
    "\n",
    "    def loss(self,left_V,right_V,bottom_V,top_V,PDE_V):      \n",
    "        return self.loss_PDE(PDE_V) + self.loss_BC(left_V,right_V,bottom_V,top_V)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fd5c09",
   "metadata": {},
   "source": [
    "# Generate initial physical mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2300dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_new = FCN2(layers).to(device)\n",
    "optimizer = torch.optim.Adam(xi_new.parameters(),lr=lr,amsgrad=False)\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = xi_new.loss(left_V,right_V,bottom_V,top_V,PDE_V)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(xi_new.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    print(i,loss.detach().cpu().numpy())\n",
    "    \n",
    "V.requires_grad = True\n",
    "V_xi_new = xi_new(V_xi_0)\n",
    "V_xi_new_cpu = V_xi_new.detach().cpu()\n",
    "plt.triplot(V_xi_new_cpu[:,0],V_xi_new_cpu[:,1],tris.detach().cpu(),'k-',linewidth=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00547fd",
   "metadata": {},
   "source": [
    "# Update physical mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6986c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi_x_y = autograd.grad(V_xi_new[:,0].unsqueeze(1),V,torch.ones([V.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "eta_x_y = autograd.grad(V_xi_new[:,1].unsqueeze(1),V,torch.ones([V.shape[0],1]).to(device),retain_graph=True,create_graph=True)[0]\n",
    "xi_x,xi_y,eta_x,eta_y = xi_x_y[:,[0]],xi_x_y[:,[1]],eta_x_y[:,[0]],eta_x_y[:,[1]]\n",
    "delta_V_xi = V_xi_0 - V_xi_new\n",
    "delta_V_xi_x = delta_V_xi[:,0].unsqueeze(1)\n",
    "delta_V_xi_y = delta_V_xi[:,1].unsqueeze(1)\n",
    "delta_V_x = (1/(xi_x*eta_y-xi_y*eta_x)*(eta_y*delta_V_xi_x-xi_y*delta_V_xi_y)).squeeze()\n",
    "delta_V_y = (1/(xi_x*eta_y-xi_y*eta_x)*(-eta_x*delta_V_xi_x+xi_x*delta_V_xi_y)).squeeze()\n",
    "delta_V = torch.stack([delta_V_x,delta_V_y],dim=1)\n",
    "delta_V = delta_V / torch.linalg.norm(delta_V,dim=1).unsqueeze(1)*0.01\n",
    "delta_V[V[:,0]==0.0] = 0.0\n",
    "delta_V[V[:,0]==1.0] = 0.0\n",
    "delta_V[V[:,1]==0.0] = 0.0\n",
    "delta_V[V[:,1]==1.0] = 0.0\n",
    "\n",
    "new_V = (V + delta_V).detach().cpu()\n",
    "\n",
    "plt.triplot(new_V[:,0],new_V[:,1],tris.detach().cpu(),'k-',linewidth=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
